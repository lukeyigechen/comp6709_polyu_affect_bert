{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy3GeHeUXuBA",
        "outputId": "30c39711-9672-4f58-881e-cb19af322fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "id": "Uy3GeHeUXuBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "unusual-albert"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertModel, RobertaForSequenceClassification, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, BertTokenizer, BertTokenizerFast, BertConfig, RobertaTokenizer\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.special import softmax, expit\n",
        "\n",
        "import os\n",
        "#os.environ['CUDA_VISIBLE_DEVICES']='1'"
      ],
      "id": "unusual-albert"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CKU1mkN1gMiP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "seed_val = 34\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "setup_seed(seed_val)"
      ],
      "id": "CKU1mkN1gMiP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dq6XZGo1gerZ",
        "outputId": "f3adf5bf-aa67-43ed-f5bf-2c34566c9be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "id": "dq6XZGo1gerZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNVlrh6gXwb0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -av /content/drive/MyDrive/PolyU/comp6709/SemEval2018-Task1-all-data/English/V-oc /content/data"
      ],
      "id": "xNVlrh6gXwb0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "td3UCVXqf4Pa"
      },
      "outputs": [],
      "source": [
        "train_df_voc = pd.read_csv('data/2018-Valence-oc-En-train.txt', sep='\\t')[['Tweet', 'Intensity Class']]\n",
        "test_df_voc = pd.read_csv('data/2018-Valence-oc-En-test-gold.txt', sep='\\t')[['Tweet', 'Intensity Class']]\n",
        "valid_df_voc = pd.read_csv('data/2018-Valence-oc-En-dev.txt', sep='\\t')[['Tweet', 'Intensity Class']]"
      ],
      "id": "td3UCVXqf4Pa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1e5oUAD55Mv"
      },
      "outputs": [],
      "source": [
        "train_df_voc.head()"
      ],
      "id": "I1e5oUAD55Mv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P8zLoSktGL4P"
      },
      "outputs": [],
      "source": [
        "train_df_voc['Intensity Class'].value_counts()"
      ],
      "id": "P8zLoSktGL4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU6xgf5SFOXu"
      },
      "outputs": [],
      "source": [
        "def change_label(df):\n",
        "    for i, row in df.iterrows():\n",
        "        ifor_val = df.loc[i,'Intensity Class'].split(':')[0]\n",
        "        if int(ifor_val) < 0:\n",
        "            ifor_val = 1  # neg\n",
        "        elif int(ifor_val) > 0:\n",
        "            ifor_val = 2  # pos\n",
        "        else: \n",
        "            ifor_val = 0  # neu\n",
        "        df.at[i,'Intensity Class'] = ifor_val\n",
        "    return df"
      ],
      "id": "GU6xgf5SFOXu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d76vmdQyGVCL"
      },
      "outputs": [],
      "source": [
        "train_df_voc = change_label(train_df_voc)\n",
        "valid_df_voc = change_label(valid_df_voc)\n",
        "test_df_voc = change_label(test_df_voc)\n",
        "\n",
        "train_df_voc.rename(columns={'Tweet': 'sentence', 'Intensity Class': 'label'}, inplace=True)\n",
        "valid_df_voc.rename(columns={'Tweet': 'sentence', 'Intensity Class': 'label'}, inplace=True)\n",
        "test_df_voc.rename(columns={'Tweet': 'sentence', 'Intensity Class': 'label'}, inplace=True)\n",
        "\n",
        "train_df_voc.head()"
      ],
      "id": "d76vmdQyGVCL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9uoGT_rGmSm"
      },
      "outputs": [],
      "source": [
        "train_df_voc['label'].value_counts()"
      ],
      "id": "I9uoGT_rGmSm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqpARv3Hgzru"
      },
      "outputs": [],
      "source": [
        "def make_dataset(df, tokenizer):\n",
        "  dataset_train = Dataset.from_pandas(df)\n",
        "  dataset_train = dataset_train.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
        "  dataset_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "  return dataset_train"
      ],
      "id": "EqpARv3Hgzru"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upper-right"
      },
      "outputs": [],
      "source": [
        "def train_model(train_df, valid_df, model_name, dir_model):\n",
        "  print('-----train-----')\n",
        "\n",
        "  if model_name == 'bert':\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  elif model_name == 'roberta':\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  elif model_name == 'bertweet':\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "\n",
        "  dataset_train = make_dataset(train_df, tokenizer)\n",
        "  dataset_val = make_dataset(valid_df, tokenizer)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "                  output_dir='./'+dir_model,          # output directory\n",
        "                  num_train_epochs=8,              # total # of training epochs\n",
        "                  per_device_train_batch_size=32,  # batch size per device during training\n",
        "                  per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "                  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "                  weight_decay=0.01,               # strength of weight decay\n",
        "                  logging_dir='./logs_'+dir_model,            # directory for storing logs\n",
        "                  #evaluation_strategy=\"steps\",\n",
        "                  evaluation_strategy=\"epoch\",\n",
        "                  save_strategy=\"epoch\",\n",
        "                  load_best_model_at_end = True,\n",
        "                  seed=seed_val,\n",
        "                  overwrite_output_dir=True,\n",
        "  )\n",
        "\n",
        "  if model_name == 'bert':\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "  elif model_name == 'roberta':\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
        "  elif model_name == 'bertweet':\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=3)\n",
        "\n",
        "  #model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "      args=training_args,                  # training arguments, defined above\n",
        "      train_dataset=dataset_train,         # training dataset\n",
        "      eval_dataset=dataset_val,            # evaluation dataset\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  \n",
        "  return tokenizer, trainer\n"
      ],
      "id": "upper-right"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhqEG8FzSll0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_fscore_support"
      ],
      "id": "HhqEG8FzSll0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FphVMfIBhlsN"
      },
      "outputs": [],
      "source": [
        "def eval_model(trainer, test_df, tokenizer, overall_types):\n",
        "  print('-----eval-----')\n",
        "  dataset_test = make_dataset(test_df, tokenizer)\n",
        "  predict_data = trainer.predict(dataset_test)\n",
        "  metrics = predict_data.metrics\n",
        "  print(metrics)\n",
        "  #print(np.argmax(predict_data.predictions, axis=1).flatten())\n",
        "  pre_labels = np.argmax(predict_data.predictions, axis=1).flatten()\n",
        "  test_df['predictions'] = pd.Series(pre_labels)\n",
        "\n",
        "  #write_out(overall_types, metrics)\n",
        "  y_true = test_df['label'].values.tolist()\n",
        "  y_pred = test_df['predictions'].values.tolist()\n",
        "  report = classification_report(y_true, y_pred)\n",
        "  print(report)\n",
        "\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  print('precision: ' + str(precision))\n",
        "  print('recall: ' + str(recall))\n",
        "  print('f1: ' + str(f1))\n",
        "  print('accuracy: ' + str(acc))\n",
        "\n",
        "  return metrics, test_df"
      ],
      "id": "FphVMfIBhlsN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crucial-monitor"
      },
      "outputs": [],
      "source": [
        "# voc, bert\n",
        "\n",
        "tokenizer, trainer_voc = train_model(train_df_voc, valid_df_voc, 'bert', 'voc-bert')\n",
        "\n",
        "metrics_voc, pred_df_voc = eval_model(trainer_voc, test_df_voc, tokenizer, 'voc-bert')\n",
        "pred_df_voc.to_csv('pred_df_voc-bert.csv', index=False)\n",
        "\n",
        "trainer_voc.save_model('./model_save_voc-bert')"
      ],
      "id": "crucial-monitor"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3gsC9_zXPTa"
      },
      "outputs": [],
      "source": [
        "# voc, roberta\n",
        "\n",
        "tokenizer, trainer_voc = train_model(train_df_voc, valid_df_voc, 'roberta', 'voc-roberta')\n",
        "\n",
        "metrics_voc, pred_df_voc = eval_model(trainer_voc, test_df_voc, tokenizer, 'voc-roberta')\n",
        "pred_df_voc.to_csv('pred_df_voc-roberta.csv', index=False)\n",
        "\n",
        "trainer_voc.save_model('./model_save_voc-roberta')"
      ],
      "id": "D3gsC9_zXPTa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1T9gZPQaN97"
      },
      "outputs": [],
      "source": [
        "# voc, bertweet\n",
        "\n",
        "tokenizer, trainer_voc = train_model(train_df_voc, valid_df_voc, 'bertweet', 'voc-bertweet')\n",
        "\n",
        "metrics_voc, pred_df_voc = eval_model(trainer_voc, test_df_voc, tokenizer, 'voc-bertweet')\n",
        "pred_df_voc.to_csv('pred_df_voc-bertweet.csv', index=False)\n",
        "\n",
        "trainer_voc.save_model('./model_save_voc-bertweet')"
      ],
      "id": "l1T9gZPQaN97"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}